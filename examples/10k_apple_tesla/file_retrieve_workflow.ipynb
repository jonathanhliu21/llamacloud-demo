{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File-level and Chunk-Level Retrieval with LlamaCloud and Workflows\n",
    "\n",
    "In this notebook we will show you how to perform file-level and chunk-level retrieval with LlamaCloud using a custom router query engine and a custom agent built with [Workflows](https://docs.llamaindex.ai/en/latest/module_guides/workflow/).\n",
    "\n",
    "File-level retrieval is useful for handling user questions that require the entire document context to properly answer the question. Since only doing file-level retrieval can be slow + expensive, we also show you how to build an agent that can dynamically decide whether to do file-level or chunk-level retrieval! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install LlamaIndex, apply nest_asyncio, and set up your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index llama-index-indices-managed-llama-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<Your OpenAI API Key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents into LlamaCloud\n",
    "\n",
    "The first order of business is to download the 5 Apple and Tesla 10Ks and upload them into LlamaCloud.\n",
    "\n",
    "You can easily do this by creating a pipeline and uploading docs via the \"Files\" mode.\n",
    "\n",
    "After this is done, proceed to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download Apple \n",
    "!wget \"https://s2.q4cdn.com/470004039/files/doc_earnings/2023/q4/filing/_10-K-Q4-2023-As-Filed.pdf\" -O data/apple_2023.pdf\n",
    "!wget \"https://s2.q4cdn.com/470004039/files/doc_financials/2022/q4/_10-K-2022-(As-Filed).pdf\" -O data/apple_2022.pdf\n",
    "!wget \"https://s2.q4cdn.com/470004039/files/doc_financials/2021/q4/_10-K-2021-(As-Filed).pdf\" -O data/apple_2021.pdf\n",
    "!wget \"https://s2.q4cdn.com/470004039/files/doc_financials/2020/ar/_10-K-2020-(As-Filed).pdf\" -O data/apple_2020.pdf\n",
    "!wget \"https://www.dropbox.com/scl/fi/i6vk884ggtq382mu3whfz/apple_2019_10k.pdf?rlkey=eudxh3muxh7kop43ov4bgaj5i&dl=1\" -O data/apple_2019.pdf\n",
    "\n",
    "# download Tesla\n",
    "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000162828024002390/tsla-20231231-gen.pdf\" -O data/tesla_2023.pdf\n",
    "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000095017023001409/tsla-20221231-gen.pdf\" -O data/tesla_2022.pdf\n",
    "!wget \"https://www.dropbox.com/scl/fi/ptk83fmye7lqr7pz9r6dm/tesla_2021_10k.pdf?rlkey=24kxixeajbw9nru1sd6tg3bye&dl=1\" -O data/tesla_2021.pdf\n",
    "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000156459021004599/tsla-10k_20201231-gen.pdf\" -O data/tesla_2020.pdf\n",
    "!wget \"https://ir.tesla.com/_flysystem/s3/sec/000156459020004475/tsla-10k_20191231-gen_0.pdf\" -O data/tesla_2019.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Classes\n",
    "\n",
    "We define the `Answer` model, which is a model that stores whether to pick chunk-level retrieval or document-level retrieval, along with a reason for that choice. We will let the LLM choose given a query string, and we will ask the LLM to produce a JSON output that can be parsed by a Pydantic model.\n",
    "\n",
    "We will define the `RouterOutputParser` helper class, which parses the output from the LLM into a list of `Answer` models, which is then put into the `Answers` model that contains a list of `Answer`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from llama_index.core.bridge.pydantic import BaseModel\n",
    "from typing import List\n",
    "from llama_index.core.types import BaseOutputParser\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# tells LLM to select choices given a list\n",
    "ROUTER_PRMOPT = PromptTemplate(\n",
    "    \"Some choices are given below. It is provided in a numbered list (1 to\"\n",
    "    \" {num_choices}), where each item in the list corresponds to a\"\n",
    "    \" summary.\\n---------------------\\n{context_list}\\n---------------------\\nUsing\"\n",
    "    \" only the choices above and not prior knowledge, return the top choices\"\n",
    "    \" (no more than {max_outputs}, but only select what is needed) that are\"\n",
    "    \" most relevant to the question: '{query_str}'\\n\"\n",
    ")\n",
    "\n",
    "# tells LLM to format list of choices in a certain way\n",
    "FORMAT_STR = \"\"\"The output should be formatted as a JSON instance that conforms to \n",
    "the JSON schema below. \n",
    "\n",
    "Here is the output schema:\n",
    "{\n",
    "  \"type\": \"array\",\n",
    "  \"items\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"choice\": {\n",
    "        \"type\": \"integer\"\n",
    "      },\n",
    "      \"reason\": {\n",
    "        \"type\": \"string\"\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\n",
    "      \"choice\",\n",
    "      \"reason\"\n",
    "    ],\n",
    "    \"additionalProperties\": false\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"Answer model.\"\"\"\n",
    "\n",
    "    choice: int\n",
    "    reason: str\n",
    "\n",
    "\n",
    "class Answers(BaseModel):\n",
    "    \"\"\"List of answers model.\"\"\"\n",
    "\n",
    "    answers: List[Answer]\n",
    "\n",
    "class RouterOutputParser(BaseOutputParser):\n",
    "    \"\"\"Custom output parser.\"\"\"\n",
    "\n",
    "    def _escape_curly_braces(self, input_string: str):\n",
    "        \"\"\"Escape the brackets in the format string so contents are not treated as variables.\"\"\"\n",
    "\n",
    "        return input_string.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "    def _marshal_output_to_json(self, output: str):\n",
    "        \"\"\"Find JSON string within response.\"\"\"\n",
    "\n",
    "        output = output.strip()\n",
    "        left = output.find(\"[\")\n",
    "        right = output.find(\"]\")\n",
    "        output = output[left : right + 1]\n",
    "        return output\n",
    "\n",
    "    def parse(self, output: str) -> Answers:\n",
    "        \"\"\"Parse string\"\"\"\n",
    "\n",
    "        json_output = self._marshal_output_to_json(output)\n",
    "        json_dicts = json.loads(json_output)\n",
    "        answers = [Answer.parse_obj(json_dict) for json_dict in json_dicts]\n",
    "        return Answers(answers=answers)\n",
    "    \n",
    "    def format(self, query: str) -> str:\n",
    "        return query + \"\\n\\n\" + self._escape_curly_braces(FORMAT_STR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router Query Workflow\n",
    "\n",
    "In the code snippet below, we define a router query workflow. This workflow requires 2 events: a `ChooseQueryEngineEvent`, which chooses the document-level or chunk-retrieval query engine, and `SynthesizeAnswersEvent`, which contains the results from the query engines and synthesizes a final response.\n",
    "\n",
    "The workflow consists of the following steps:\n",
    "1. Choosing the query engine(s) by passing the prompt and output parser defined above into an LLM. Both query engines can be chosen if the LLM thinks both query engines (defined in `choose_query_engine()`).\n",
    "2. Queries the engines chosen by the LLM in the previous step (defined in `query_each_engine`).\n",
    "3. Synthesizes a final response given the results from the queries above (defined in `synthesize_response()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Any\n",
    "\n",
    "from llama_index.core.query_engine import (\n",
    "    BaseQueryEngine,\n",
    "    RetrieverQueryEngine,\n",
    ")\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import LLM\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.program import LLMTextCompletionProgram\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    ")\n",
    "\n",
    "class ChooseQueryEngineEvent(Event):\n",
    "    \"\"\"Query engine event.\"\"\"\n",
    "\n",
    "    answers: Answers\n",
    "    query_str: str\n",
    "\n",
    "class SynthesizeAnswersEvent(Event):\n",
    "    \"\"\"Synthesize answers event.\"\"\"\n",
    "\n",
    "    responses: List[Any]\n",
    "    query_str: str\n",
    "\n",
    "\n",
    "class RouterQueryWorkflow(Workflow):\n",
    "    \"\"\"Router query workflow.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_engines: List[BaseQueryEngine],\n",
    "        choice_descriptions: List[str],\n",
    "        router_prompt: PromptTemplate,\n",
    "        timeout: Optional[float] = 10.0,\n",
    "        disable_validation: bool = False,\n",
    "        verbose: bool = False,\n",
    "        llm: Optional[LLM] = None,\n",
    "        summarizer: Optional[TreeSummarize] = None,\n",
    "    ):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "\n",
    "        super().__init__(timeout=timeout, disable_validation=disable_validation, verbose=verbose)\n",
    "\n",
    "        self.query_engines: List[BaseQueryEngine] = query_engines\n",
    "        self.choice_descriptions: List[str] = choice_descriptions\n",
    "        self.router_prompt: PromptTemplate = router_prompt\n",
    "        self.llm: LLM = llm or OpenAI(temperature=0, model=\"gpt-4o\")\n",
    "        self.summarizer: TreeSummarize = summarizer or TreeSummarize()\n",
    "\n",
    "    def _get_choice_str(self, choices):\n",
    "        \"\"\"String of choices to feed into LLM.\"\"\"\n",
    "\n",
    "        choices_str = \"\\n\\n\".join([f\"{idx+1}. {c}\" for idx, c in enumerate(choices)])\n",
    "        return choices_str\n",
    "    \n",
    "    async def _query(self, query_str: str, choice_idx: int):\n",
    "        \"\"\"Query using query engine\"\"\"\n",
    "\n",
    "        query_engine = self.query_engines[choice_idx]\n",
    "        return await query_engine.aquery(query_str)\n",
    "\n",
    "    \n",
    "    @step()\n",
    "    async def choose_query_engine(self, ev: StartEvent) -> ChooseQueryEngineEvent:\n",
    "        \"\"\"Choose query engine.\"\"\"\n",
    "\n",
    "        # get query str\n",
    "        query_str = ev.get(\"query_str\")\n",
    "        if query_str is None:\n",
    "            raise ValueError(\"'query_str' is required.\")\n",
    "        \n",
    "        # partially format prompt with number of choices and max outputs\n",
    "        router_prompt1 = self.router_prompt.partial_format(\n",
    "            num_choices=len(self.choice_descriptions),\n",
    "            max_outputs=len(self.choice_descriptions),\n",
    "        )\n",
    "\n",
    "        # get structured output of answers\n",
    "        program = LLMTextCompletionProgram.from_defaults(\n",
    "            output_cls=Answers,\n",
    "            prompt=router_prompt1,\n",
    "            verbose=self._verbose,\n",
    "            llm=self.llm,\n",
    "            output_parser=RouterOutputParser()\n",
    "        )\n",
    "        # get choices selected by LLM\n",
    "        choices_str = self._get_choice_str(self.choice_descriptions)\n",
    "        output: Answers = program(context_list=choices_str, query_str=query_str)\n",
    "\n",
    "        if self._verbose:\n",
    "            print(f\"Selected choice(s):\")\n",
    "            for answer in output.answers:\n",
    "                print(f\"Choice: {answer.choice}, Reason: {answer.reason}\")\n",
    "        \n",
    "        return ChooseQueryEngineEvent(answers=output, query_str=query_str)\n",
    "            \n",
    "    @step()\n",
    "    async def query_each_engine(self, ev: ChooseQueryEngineEvent) -> SynthesizeAnswersEvent:\n",
    "        \"\"\"Query each engine.\"\"\"\n",
    "\n",
    "        query_str = ev.query_str\n",
    "        answers = ev.answers\n",
    "\n",
    "        # query using corresponding query engine given in Answers list\n",
    "        responses = []\n",
    "\n",
    "        for answer in answers.answers:\n",
    "            choice_idx = answer.choice - 1\n",
    "            response = await self._query(query_str, choice_idx)\n",
    "            responses.append(response)\n",
    "        \n",
    "        return SynthesizeAnswersEvent(responses=responses, query_str=query_str)\n",
    "    \n",
    "    @step()\n",
    "    async def synthesize_response(self, ev: SynthesizeAnswersEvent) -> StopEvent:\n",
    "        \"\"\"Synthesizes response.\"\"\"\n",
    "\n",
    "        responses = ev.responses\n",
    "        query_str = ev.query_str\n",
    "\n",
    "        # get result of responses\n",
    "        if len(responses) == 1:\n",
    "            return StopEvent(result=responses[0])\n",
    "        else:\n",
    "            response_strs = [str(r) for r in responses]\n",
    "            result_response = self.summarizer.get_response(query_str, response_strs)\n",
    "            return StopEvent(result=result_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LlamaCloud Retriever over documents\n",
    "\n",
    "We'll define an instance of `LLamaCloudIndex`, which will allow us to access the indexed docs stored on LlamaCloud. We define two separate retrievers for this index: a file-level retriever and a chunk-level retriever. We create two query engines from these retrievers.\n",
    "\n",
    "After this, we give a description for what each retriever does to allow the LLM to know which one to pick. We'll define our router workflow based on the two query engines and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "\n",
    "index = LlamaCloudIndex(\n",
    "    name=\"<Your Index Name>\",\n",
    "    project_name=\"<Your Project Name>\",\n",
    "    api_key=\"<Your API Key>\",\n",
    "    organization_id=\"<Your Org ID>\",\n",
    ")\n",
    "llm = OpenAI(\"gpt-4o\")\n",
    "\n",
    "doc_retriever = index.as_retriever(retrieval_mode=\"files_via_content\", files_top_k=1)\n",
    "query_engine_doc = RetrieverQueryEngine.from_args(\n",
    "    doc_retriever, llm=llm, response_mode=\"tree_summarize\"\n",
    ")\n",
    "\n",
    "chunk_retriever = index.as_retriever(retrieval_mode=\"chunks\", rerank_top_n=10)\n",
    "query_engine_chunk = RetrieverQueryEngine.from_args(\n",
    "    chunk_retriever, llm=llm, response_mode=\"tree_summarize\"\n",
    ")\n",
    "\n",
    "DOC_METADATA_EXTRA_STR = \"\"\"\\\n",
    "Each document represents a complete 10K report for a given year (e.g. Apple in 2019).\n",
    "Here's an example of relevant documents:\n",
    "1. apple_2019.pdf\n",
    "2. tesla_2020.pdf\n",
    "\"\"\"\n",
    "\n",
    "TOOL_DOC_DESC = f\"\"\"\\\n",
    "Synthesizes an answer to your question by feeding in an entire relevant document as context. Best used for higher-level summarization options.\n",
    "Do NOT use if answer can be found in a specific chunk of a given document. Use the chunk_query_engine instead for that purpose.\n",
    "\n",
    "Below we give details on the format of each document:\n",
    "{DOC_METADATA_EXTRA_STR}\n",
    "\"\"\"\n",
    "\n",
    "TOOL_CHUNK_DESC = f\"\"\"\\\n",
    "Synthesizes an answer to your question by feeding in a relevant chunk as context. Best used for questions that are more pointed in nature.\n",
    "Do NOT use if the question asks seems to require a general summary of any given document. Use the doc_query_engine instead for that purpose.\n",
    "\n",
    "Below we give details on the format of each document:\n",
    "{DOC_METADATA_EXTRA_STR}\n",
    "\"\"\"\n",
    "\n",
    "router_query_workflow = RouterQueryWorkflow(\n",
    "    query_engines=[query_engine_doc, query_engine_chunk],\n",
    "    choice_descriptions=[TOOL_DOC_DESC, TOOL_CHUNK_DESC],\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    router_prompt=ROUTER_PRMOPT,\n",
    "    timeout=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining our router query workflow, we'll create a query engine wrapper around this workflow, and we'll define a query engine tool around this wrapper to pass into an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.async_utils import asyncio_run\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "class RouterQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"Router query engine (for tool usage).\"\"\"\n",
    "\n",
    "    router_query_workflow: RouterQueryWorkflow\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        \"\"\"Query.\"\"\"\n",
    "        \n",
    "        return asyncio_run(router_query_workflow.run(query_str=query_str))\n",
    "    \n",
    "router_query_engine = RouterQueryEngine(router_query_workflow=router_query_workflow)\n",
    "router_query_engine_tool = QueryEngineTool(router_query_engine, metadata=ToolMetadata(\n",
    "    name=\"Query_engine\",\n",
    "    description=\"Queries 10k reports for a given year.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Agent Around the Query Engine\n",
    "\n",
    "We'll create a workflow that acts as an agent around the router query engine. In this workflow, we need four events:\n",
    "1. `GatherToolsEvent`: Gets all tools that need to be called (which is determined by the LLM).\n",
    "2. `ToolCallEvent`: An individual tool call. Multiple of these events will be triggered at the same time.\n",
    "3. `ToolCallEventResult`: Gets result from a tool call.\n",
    "4. `GatherEvent`: Returned from dispatcher that triggers the `ToolCallEvent`.\n",
    "\n",
    "This workflow consists of the following steps:\n",
    "1. `chat()`: Appends the message to the chat history. This chat history is fed into the LLM, along with the given tools, and the LLM determines which tools to call. This returns a `GatherToolsEvent`.\n",
    "2. `dispatch_calls()`: Triggers a `ToolCallEvent` for each tool call given in the `GatherToolsEvent` using `send_event()`. Returns a `GatherEvent` with the number of tool calls.\n",
    "3. `call_tool()`: Calls an individual tool. This step will run multiple times if there is more than one tool call. This step calls the tool and appends the result as a chat message to the chat history. It returns a `ToolCallEventResult` with the result of the tool call.\n",
    "4. `gather()`: Gathers the results from all tool calls using `collect_events()`. Waits for all tool calls to finish, then feeds chat history (following all tool calls) into the LLM. Returns the response from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from llama_index.core.tools import BaseTool\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.workflow import Context\n",
    "from openai.types.chat import ChatCompletionMessageToolCall\n",
    "\n",
    "class GatherToolsEvent(Event):\n",
    "    \"\"\"Gather Tools Event\"\"\"\n",
    "\n",
    "    tool_calls: Any\n",
    "    message: str\n",
    "\n",
    "class ToolCallEvent(Event):\n",
    "    \"\"\"Tool Call event\"\"\"\n",
    "\n",
    "    tool_call: ChatCompletionMessageToolCall\n",
    "\n",
    "class ToolCallEventResult(Event):\n",
    "    \"\"\"Tool call event result.\"\"\"\n",
    "\n",
    "    msg: ChatMessage\n",
    "\n",
    "class GatherEvent(Event):\n",
    "    \"\"\"Gather event\"\"\"\n",
    "\n",
    "    num_tool_calls: int\n",
    "\n",
    "class RouterOutputAgentWorkflow(Workflow):\n",
    "    \"\"\"Custom router output agent workflow.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        tools: List[BaseTool],\n",
    "        timeout: Optional[float] = 10.0,\n",
    "        disable_validation: bool = False,\n",
    "        verbose: bool = False,\n",
    "        llm: Optional[LLM] = None,\n",
    "        chat_history: Optional[List[ChatMessage]] = None,\n",
    "    ):\n",
    "        \"\"\"Constructor.\"\"\"\n",
    "\n",
    "        super().__init__(timeout=timeout, disable_validation=disable_validation, verbose=verbose)\n",
    "\n",
    "        self.tools: List[BaseTool] = tools\n",
    "        self.tools_dict: Optional[Dict[str, BaseTool]] = {tool.metadata.name: tool for tool in self.tools}\n",
    "        self.llm: LLM = llm or OpenAI(temperature=0, model=\"gpt-4o\")\n",
    "        self.chat_history: List[ChatMessage] = chat_history or []\n",
    "    \n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets Chat History\"\"\"\n",
    "\n",
    "        self.chat_history = []\n",
    "\n",
    "    @step()\n",
    "    async def chat(self, ev: StartEvent) -> GatherToolsEvent | StopEvent:\n",
    "        \"\"\"Appends msg to chat history, then gets tool calls.\"\"\"\n",
    "\n",
    "        message = ev.get(\"message\")\n",
    "        if message is None:\n",
    "            raise ValueError(\"'message' field is required.\")\n",
    "        \n",
    "        # add msg to chat history\n",
    "        chat_history = self.chat_history\n",
    "        chat_history.append(ChatMessage(role=\"user\", content=message))\n",
    "\n",
    "        # Put msg into LLM with tools included\n",
    "        tools = [tool.metadata.to_openai_tool() for tool in self.tools]\n",
    "        chat_res = await self.llm.achat(chat_history, tools=tools)\n",
    "        ai_message = chat_res.message\n",
    "        additional_kwargs = ai_message.additional_kwargs\n",
    "        chat_history.append(ai_message)\n",
    "\n",
    "        if self._verbose:\n",
    "            print(f\"Chat message: {ai_message.content}\")\n",
    "\n",
    "        # get tool calls\n",
    "        tool_calls = additional_kwargs.get(\"tool_calls\", None)\n",
    "\n",
    "        # no tool calls, return chat message.\n",
    "        if tool_calls is None:\n",
    "            return StopEvent(result=ai_message.content)\n",
    "\n",
    "        return GatherToolsEvent(tool_calls=tool_calls, message=message)\n",
    "\n",
    "    @step()\n",
    "    async def dispatch_calls(self, ev: GatherToolsEvent) -> ToolCallEvent | GatherEvent:\n",
    "        \"\"\"Dispatches calls.\"\"\"\n",
    "\n",
    "        tool_calls = ev.tool_calls\n",
    "\n",
    "        # trigger tool call events\n",
    "        for tool_call in tool_calls:\n",
    "            self.send_event(ToolCallEvent(tool_call=tool_call))\n",
    "        \n",
    "        return GatherEvent(num_tool_calls=len(tool_calls))\n",
    "    \n",
    "    @step()\n",
    "    async def call_tool(self, ev: ToolCallEvent) -> ToolCallEventResult:\n",
    "        \"\"\"Calls tool.\"\"\"\n",
    "\n",
    "        tool_call = ev.tool_call\n",
    "\n",
    "        # get tool ID and function call\n",
    "        id_ = tool_call.id\n",
    "        function_call = tool_call.function\n",
    "\n",
    "        if self._verbose:\n",
    "            print(f\"Calling function {function_call.name} with msg {function_call.arguments}\")\n",
    "\n",
    "        # call function and put result into a chat message\n",
    "        tool = self.tools_dict[function_call.name]\n",
    "        output = await tool.acall(**json.loads(function_call.arguments))\n",
    "        msg = ChatMessage(\n",
    "            name=function_call.name,\n",
    "            content=str(output),\n",
    "            role=\"tool\",\n",
    "            additional_kwargs={\n",
    "                \"tool_call_id\": id_,\n",
    "                \"name\": function_call.name\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # append chat message to history\n",
    "        self.chat_history.append(msg)\n",
    "\n",
    "        return ToolCallEventResult(msg=msg)\n",
    "    \n",
    "    @step(pass_context=True)\n",
    "    async def gather(self, ctx: Context, ev: GatherEvent | ToolCallEventResult) -> StopEvent | None:\n",
    "        \"\"\"Gathers tool calls.\"\"\"\n",
    "\n",
    "        if isinstance(ev, GatherEvent):\n",
    "            ctx.data[\"num_tool_calls\"] = ev.num_tool_calls\n",
    "\n",
    "        # wait for all tool call events to finish.\n",
    "        events = ctx.collect_events(ev, [ToolCallEventResult] * ctx.data[\"num_tool_calls\"])\n",
    "        if not events:\n",
    "            return None\n",
    "        \n",
    "        # after all tool calls finish, pass chat history into LLM\n",
    "        # and return result.\n",
    "        ai_message = self.llm.chat(self.chat_history).message\n",
    "        self.chat_history.append(ai_message)\n",
    "\n",
    "        return StopEvent(result=ai_message.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates an instance of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RouterOutputAgentWorkflow(tools=[router_query_engine_tool], verbose=True, timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step chat\n",
      "Chat message: None\n",
      "Step chat produced event GatherToolsEvent\n",
      "Running step dispatch_calls\n",
      "Step dispatch_calls produced event GatherEvent\n",
      "Running step gather\n",
      "Step gather produced no event\n",
      "Running step call_tool\n",
      "Calling function Query_engine with msg {\"input\": \"Apple 2021 revenue\"}\n",
      "Running step choose_query_engine\n",
      "Selected choice(s):\n",
      "Choice: 2, Reason: The question 'Apple 2021 revenue' is pointed in nature and likely can be answered by extracting specific information from a relevant chunk of the document. Therefore, using the chunk_query_engine is appropriate.\n",
      "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
      "Running step query_each_engine\n",
      "Step query_each_engine produced event SynthesizeAnswersEvent\n",
      "Running step synthesize_response\n",
      "Step synthesize_response produced event StopEvent\n",
      "Step call_tool produced event ToolCallEventResult\n",
      "Running step call_tool\n",
      "Calling function Query_engine with msg {\"input\": \"Tesla 2021 revenue\"}\n",
      "Running step gather\n",
      "Step gather produced no event\n",
      "Running step choose_query_engine\n",
      "Selected choice(s):\n",
      "Choice: 2, Reason: The question 'Tesla 2021 revenue' is pointed in nature and likely can be answered by extracting specific information from a relevant chunk of the document. Therefore, using the chunk_query_engine is more appropriate.\n",
      "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
      "Running step query_each_engine\n",
      "Step query_each_engine produced event SynthesizeAnswersEvent\n",
      "Running step synthesize_response\n",
      "Step synthesize_response produced event StopEvent\n",
      "Step call_tool produced event ToolCallEventResult\n",
      "Running step gather\n",
      "Step gather produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "In 2021, Apple reported total net sales of $365.8 billion, which included $297.4 billion from products and $68.4 billion from services. Tesla, on the other hand, reported total revenue of $53.82 billion for the same year."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "response = await agent.run(message=\"Tell me the revenue for Apple and Tesla in 2021.\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step chat\n",
      "Chat message: None\n",
      "Step chat produced event GatherToolsEvent\n",
      "Running step dispatch_calls\n",
      "Step dispatch_calls produced event GatherEvent\n",
      "Running step gather\n",
      "Step gather produced no event\n",
      "Running step call_tool\n",
      "Calling function Query_engine with msg {\"input\": \"Apple 2021 tailwinds\"}\n",
      "Running step choose_query_engine\n",
      "Selected choice(s):\n",
      "Choice: 2, Reason: The question 'Apple 2021 tailwinds' is pointed in nature and likely requires specific information from a particular section of the 2021 10K report. Therefore, using the chunk_query_engine to find the relevant chunk is the best approach.\n",
      "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
      "Running step query_each_engine\n",
      "Step query_each_engine produced event SynthesizeAnswersEvent\n",
      "Running step synthesize_response\n",
      "Step synthesize_response produced event StopEvent\n",
      "Step call_tool produced event ToolCallEventResult\n",
      "Running step call_tool\n",
      "Calling function Query_engine with msg {\"input\": \"Tesla 2021 tailwinds\"}\n",
      "Running step gather\n",
      "Step gather produced no event\n",
      "Running step choose_query_engine\n",
      "Selected choice(s):\n",
      "Choice: 2, Reason: The question 'Tesla 2021 tailwinds' is pointed in nature and likely requires specific information from a relevant chunk of the document rather than a general summary.\n",
      "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
      "Running step query_each_engine\n",
      "Step query_each_engine produced event SynthesizeAnswersEvent\n",
      "Running step synthesize_response\n",
      "Step synthesize_response produced event StopEvent\n",
      "Step call_tool produced event ToolCallEventResult\n",
      "Running step gather\n",
      "Step gather produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "In 2021, both Apple and Tesla experienced several tailwinds that contributed to their strong performance:\n",
       "\n",
       "### Apple:\n",
       "1. **Higher Net Sales**: Increased sales of iPhone, Services, and Mac across various regions, including the Americas, Europe, Greater China, Japan, and the Rest of Asia Pacific.\n",
       "2. **Favorable Currency Movements**: The favorable movement of foreign currencies relative to the U.S. dollar positively impacted net sales in Europe, Greater China, and the Rest of Asia Pacific.\n",
       "3. **Increased Gross Margin**: Higher product volumes, a different product mix, and improved leverage contributed to an increase in gross margin. The strength in foreign currencies relative to the U.S. dollar also supported the increase in gross margin percentage.\n",
       "\n",
       "### Tesla:\n",
       "1. **Increased Revenues**: Total revenues rose by 71% compared to the prior year, driven by ramped-up production and expanded manufacturing capacity, enabling increased deliveries and deployments of their products.\n",
       "2. **Improved Net Income**: Net income attributable to common stockholders saw a favorable change, increasing by $4.80 billion.\n",
       "3. **Enhanced Cash Flows**: Cash flows provided by operating activities increased by $5.55 billion.\n",
       "4. **Operational Efficiencies**: Focus on production and operational efficiencies.\n",
       "5. **Market Trends**: Ongoing electrification of the automotive sector and increasing environmental awareness supported Tesla's growth.\n",
       "\n",
       "These factors collectively helped both companies achieve significant financial and operational success in 2021."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await agent.run(message=\"Tell me the tailwinds for Apple and Tesla in 2021.\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step chat\n",
      "Chat message: None\n",
      "Step chat produced event GatherToolsEvent\n",
      "Running step dispatch_calls\n",
      "Step dispatch_calls produced event GatherEvent\n",
      "Running step gather\n",
      "Step gather produced no event\n",
      "Running step call_tool\n",
      "Calling function Query_engine with msg {\"input\":\"Apple 2019 performance\"}\n",
      "Running step choose_query_engine\n",
      "Selected choice(s):\n",
      "Choice: 1, Reason: The question 'Apple 2019 performance' requires a general summary of the entire document, which is best handled by synthesizing an answer using the entire relevant document as context.\n",
      "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
      "Running step query_each_engine\n",
      "Step query_each_engine produced event SynthesizeAnswersEvent\n",
      "Running step synthesize_response\n",
      "Step synthesize_response produced event StopEvent\n",
      "Step call_tool produced event ToolCallEventResult\n",
      "Running step gather\n",
      "Step gather produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "In 2019, Apple Inc. experienced a mixed performance:\n",
       "\n",
       "### Financial Highlights:\n",
       "- **Total Net Sales**: $260.2 billion, a 2% decrease compared to 2018.\n",
       "- **iPhone Sales**: Declined by 14% to $142.4 billion.\n",
       "- **Wearables, Home and Accessories**: Increased by 41% to $24.5 billion.\n",
       "- **Services Sales**: Rose by 16% to $46.3 billion.\n",
       "- **Operating Income**: $63.9 billion.\n",
       "- **Net Income**: $55.3 billion.\n",
       "\n",
       "### Investments and Shareholder Returns:\n",
       "- **Research and Development**: Expenses increased by 14% to $16.2 billion.\n",
       "- **Stock Repurchase**: Apple repurchased $67.1 billion of its common stock.\n",
       "- **Dividends**: Paid $14.1 billion in dividends and dividend equivalents.\n",
       "\n",
       "### Summary:\n",
       "Despite a decline in overall net sales, primarily due to lower iPhone sales, Apple saw significant growth in its Wearables, Home and Accessories, and Services segments. The company continued to invest heavily in research and development and returned substantial value to shareholders through stock repurchases and dividends."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await agent.run(message=\"How was apple doing generally in 2019?\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Advanced] Setup Auto-Retrieval for Files\n",
    "\n",
    "We make our file-level retrieval more sophisticated by allowing the LLM to infer a set of metadata filters, based on some relevant example documents. This allows document-level retrieval to be more precise, since it allows the LLM to narrow down search results via metadata filters and not just top-k.\n",
    "\n",
    "We do some advanced things to make this happen\n",
    "- Define a custom prompt to generate metadata filters\n",
    "- Dynamically include few-shot examples of metadata as context to infer the set of metadata filters. These initial few-shot examples of metadata are obtained through vector search.\n",
    "\n",
    "We prompt the LLM to generate a prompt, a list of filters, and optionally a top-k value. We will define another workflow that is subclassed from the `RouterQueryWorkflow`. In this workflow, we will replace the `_query()` method defined in `RouterQueryWorkflow`.\n",
    "\n",
    "In this `_query()` method, we will check if the choice is the document-level retrieval. If it is, then we'll create a new query engine with certain LLM-generated filters applied. We'll return the response from this query engine.\n",
    "\n",
    "A lot of the code below is lifted from our **VectorIndexAutoRetriever** module, which provides an out of the box way to do auto-retrieval against a vector index.\n",
    "\n",
    "Since we are adding some customizations like adding few-shot examples, we re-use prompt pieces and implement auto-retrieval from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "from llama_index.core.vector_stores.types import (\n",
    "    VectorStoreInfo,\n",
    "    VectorStoreQuerySpec,\n",
    "    MetadataInfo,\n",
    "    MetadataFilters,\n",
    ")\n",
    "\n",
    "SYS_PROMPT = \"\"\"\\\n",
    "Your goal is to structure the user's query to match the request schema provided below.\n",
    "\n",
    "<< Structured Request Schema >>\n",
    "When responding use a markdown code snippet with a JSON object formatted in the \\\n",
    "following schema:\n",
    "\n",
    "{schema_str}\n",
    "\n",
    "The query string should contain only text that is expected to match the contents of \\\n",
    "documents. Any conditions in the filter should not be mentioned in the query as well.\n",
    "\n",
    "Make sure that filters only refer to attributes that exist in the data source.\n",
    "Make sure that filters take into account the descriptions of attributes.\n",
    "Make sure that filters are only used as needed. If there are no filters that should be \\\n",
    "applied return [] for the filter value.\\\n",
    "\n",
    "If the user's query explicitly mentions number of documents to retrieve, set top_k to \\\n",
    "that number, otherwise do not set top_k.\n",
    "\n",
    "The schema of the metadata filters in the vector db table is listed below, along with some example metadata dictionaries from relevant rows.\n",
    "The user will send the input query string.\n",
    "\n",
    "Data Source:\n",
    "```json\n",
    "{info_str}\n",
    "```\n",
    "\n",
    "Example metadata from relevant chunks:\n",
    "{example_rows}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AutoRetrievalRouterQueryWorkflow(RouterQueryWorkflow):\n",
    "    \"\"\"Router query engine with auto retrieval.\"\"\"\n",
    "\n",
    "    async def _get_auto_retriever_query_engine(\n",
    "        self, query: str, verbose: bool = False\n",
    "    ) -> RetrieverQueryEngine:\n",
    "        \"\"\"Gets auto doc retriever query engine\"\"\"\n",
    "\n",
    "        # retriever that retrieves example rows\n",
    "        example_rows_retriever = index.as_retriever(\n",
    "            retrieval_mode=\"chunks\", rerank_top_n=4\n",
    "        )\n",
    "\n",
    "        def get_example_rows_fn(**kwargs):\n",
    "            \"\"\"Retrieve relevant few-shot examples of metadata.\"\"\"\n",
    "\n",
    "            query_str = kwargs[\"query_str\"]\n",
    "            nodes = example_rows_retriever.retrieve(query_str)\n",
    "            # get the metadata, join them\n",
    "            metadata_list = [n.metadata for n in nodes]\n",
    "\n",
    "            return \"\\n\".join([json.dumps(m) for m in metadata_list])\n",
    "\n",
    "        # define chat prompt template to feed into LLM\n",
    "        chat_prompt_tmpl = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", SYS_PROMPT),\n",
    "                (\"user\", \"{query_str}\"),\n",
    "            ],\n",
    "            function_mappings={\"example_rows\": get_example_rows_fn},\n",
    "        )\n",
    "\n",
    "        # information about vector store - used to generate json schema in prompt template\n",
    "        vector_store_info = VectorStoreInfo(\n",
    "            content_info=\"document chunks around Apple and Tesla 10K documents\",\n",
    "            metadata_info=[\n",
    "                MetadataInfo(\n",
    "                    name=\"file_name\",\n",
    "                    type=\"str\",\n",
    "                    description=\"Name of the source file\",\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # use structured output to get metadata filters and query str\n",
    "        program = LLMTextCompletionProgram.from_defaults(\n",
    "            output_cls=VectorStoreQuerySpec,\n",
    "            prompt=chat_prompt_tmpl,\n",
    "            llm=self.llm,\n",
    "            verbose=self._verbose,\n",
    "        )\n",
    "        query_spec: VectorStoreQuerySpec = await program.acall(\n",
    "            info_str=vector_store_info.json(indent=4),\n",
    "            schema_str=VectorStoreQuerySpec.schema_json(indent=4),\n",
    "            query_str=query,\n",
    "        )\n",
    "\n",
    "        # build retriever and query engine\n",
    "        filters = (\n",
    "            MetadataFilters(filters=query_spec.filters)\n",
    "            if len(query_spec.filters) > 0\n",
    "            else None\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"> Using query str: {query_spec.query}\")\n",
    "\n",
    "        if filters and verbose:\n",
    "            print(f\"> Using filters{filters.json()}\")\n",
    "\n",
    "        retriever = index.as_retriever(\n",
    "            retrieval_mode=\"files_via_content\", files_top_k=1, filters=filters\n",
    "        )\n",
    "\n",
    "        query_engine = RetrieverQueryEngine.from_args(\n",
    "            retriever, llm=self.llm, response_mode=\"tree_summarize\"\n",
    "        )\n",
    "\n",
    "        return query_engine\n",
    "\n",
    "    async def _query(self, query_str: str, choice_idx: int):\n",
    "        \"\"\"Query with auto retriever\"\"\"\n",
    "\n",
    "        if choice_idx == 0:\n",
    "            query_engine = await self._get_auto_retriever_query_engine(\n",
    "                query_str, self._verbose\n",
    "            )\n",
    "        else:\n",
    "            query_engine = self.query_engines[choice_idx]\n",
    "        return await query_engine.aquery(query_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the auto retrieval query workflow, then wrap it around a RouterQueryEngine, then create a tool around that engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto retrieval query engine\n",
    "auto_retrieval_query_workflow = AutoRetrievalRouterQueryWorkflow(\n",
    "    query_engines=[query_engine_doc, query_engine_chunk],\n",
    "    choice_descriptions=[TOOL_DOC_DESC, TOOL_CHUNK_DESC],\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    router_prompt=ROUTER_PRMOPT,\n",
    ")\n",
    "\n",
    "# auto retrieval query engine\n",
    "auto_retrieval_query_engine = RouterQueryEngine(\n",
    "    router_query_workflow=auto_retrieval_query_workflow\n",
    ")\n",
    "\n",
    "# create tool\n",
    "auto_retrieval_query_engine_tool = QueryEngineTool(\n",
    "    query_engine=auto_retrieval_query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"Query_engine_auto_retrieval\",\n",
    "        description=\"Queries 10k reports for a given year.\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an agent using auto retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent\n",
    "agent_router_output = RouterOutputAgentWorkflow(tools=[auto_retrieval_query_engine_tool], verbose=True, timeout=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step chat\n",
      "Chat message: None\n",
      "Step chat produced event GatherToolsEvent\n",
      "Running step dispatch_calls\n",
      "Step dispatch_calls produced event GatherEvent\n",
      "Running step gather\n",
      "Step gather produced no event\n",
      "Running step call_tool\n",
      "Calling function Query_engine_auto_retrieval with msg {\"input\": \"Tesla 2021\"}\n",
      "Running step choose_query_engine\n",
      "Selected choice(s):\n",
      "Choice: 1, Reason: The question 'Tesla 2021' seems to require a general summary of the 10K report for Tesla in 2021. Choice 1 is best suited for higher-level summarization options and synthesizes an answer by feeding in an entire relevant document as context.\n",
      "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
      "Running step query_each_engine\n",
      "Step query_each_engine produced event SynthesizeAnswersEvent\n",
      "Running step synthesize_response\n",
      "Step synthesize_response produced event StopEvent\n",
      "Step call_tool produced event ToolCallEventResult\n",
      "Running step call_tool\n",
      "Calling function Query_engine_auto_retrieval with msg {\"input\": \"Tesla 2022\"}\n",
      "Running step gather\n",
      "Step gather produced no event\n",
      "Running step choose_query_engine\n",
      "Selected choice(s):\n",
      "Choice: 1, Reason: The question 'Tesla 2022' seems to require a general summary of the 10K report for Tesla in 2022. Choice 1 is best suited for higher-level summarization options and synthesizes an answer by feeding in an entire relevant document as context.\n",
      "Step choose_query_engine produced event ChooseQueryEngineEvent\n",
      "Running step query_each_engine\n",
      "Step query_each_engine produced event SynthesizeAnswersEvent\n",
      "Running step synthesize_response\n",
      "Step synthesize_response produced event StopEvent\n",
      "Step call_tool produced event ToolCallEventResult\n",
      "Running step gather\n",
      "Step gather produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Tesla in 2021\n",
       "\n",
       "In 2021, Tesla experienced significant growth and made strategic investments to bolster its financial and operational capabilities:\n",
       "\n",
       "1. **Investment in Bitcoin**: Tesla updated its investment policy to allow for more flexibility in diversifying and maximizing returns on its cash reserves. This included a notable $1.5 billion investment in bitcoin and plans to accept bitcoin as a form of payment.\n",
       "\n",
       "2. **Convertible Senior Notes**: The company received conversion notices for its 2022 and 2024 convertible senior notes, which it intended to settle in cash during the first quarter of 2021. Additionally, Tesla issued 2.00% Convertible Senior Notes due May 15, 2024.\n",
       "\n",
       "3. **Partnerships and Agreements**: Tesla entered into a 2021 Pricing Agreement for Japan Cells with Panasonic Corporation of North America and SANYO Electric Co., Ltd., highlighting its ongoing efforts to secure essential components for its electric vehicles.\n",
       "\n",
       "4. **Expansion in Electric Vehicle and Clean Energy Sectors**: Tesla continued to grow in the electric vehicle and clean energy sectors, managing various financial and operational activities, including securing loans and managing assets through agreements with financial institutions.\n",
       "\n",
       "### Tesla in 2022\n",
       "\n",
       "In 2022, Tesla built on its previous successes and achieved significant milestones:\n",
       "\n",
       "1. **Vehicle Production and Delivery**: Tesla produced and delivered a substantial number of vehicles, focusing on increasing production capacity and efficiency to meet growing demand.\n",
       "\n",
       "2. **Revenue Growth**: The company's revenue for 2022 was $81.46 billion, marking a significant increase from previous years.\n",
       "\n",
       "3. **Energy Generation and Storage**: Tesla made strides in its energy generation and storage segment, deploying more energy storage products and solar energy systems.\n",
       "\n",
       "4. **Global Infrastructure Expansion**: Tesla continued to expand its global infrastructure, including service centers and Supercharger stations, to support its growing customer base.\n",
       "\n",
       "Overall, Tesla's performance in 2021 and 2022 reflected its ongoing commitment to innovation, strategic investments, and expansion in both the electric vehicle and clean energy markets."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await agent_router_output.run(message=\"How was Tesla doing generally in 2021 and 2022?\")\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
